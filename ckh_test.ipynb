{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from alphafold3_pytorch import collate_inputs_to_batched_atom_input\n",
    "from alphafold3_pytorch.alphafold3 import Alphafold3\n",
    "from alphafold3_pytorch.inputs import (\n",
    "    PDBDataset,\n",
    "    molecule_to_atom_input,\n",
    "    pdb_input_to_molecule_input,\n",
    ")\n",
    "from alphafold3_pytorch.data.weighted_pdb_sampler import WeightedPDBSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-04 08:15:13.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36malphafold3_pytorch.data.weighted_pdb_sampler\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mPrecomputing chain and interface weights. This may take several minutes to complete.\u001b[0m\n",
      "\u001b[32m2024-09-04 08:15:13.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36malphafold3_pytorch.data.weighted_pdb_sampler\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m258\u001b[0m - \u001b[1mFinished precomputing chain and interface weights.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_test = os.path.join(\"data\", \"test\")\n",
    "data_test = os.path.join('/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data/data_caches/200_mmcif')\n",
    "\n",
    "\"\"\"Test a PDBDataset constructed using a WeightedPDBSampler.\"\"\"\n",
    "interface_mapping_path = os.path.join(data_test, \"interface_cluster_mapping.csv\")\n",
    "chain_mapping_paths = [\n",
    "    os.path.join(data_test, \"ligand_chain_cluster_mapping.csv\"),\n",
    "    os.path.join(data_test, \"nucleic_acid_chain_cluster_mapping.csv\"),\n",
    "    os.path.join(data_test, \"peptide_chain_cluster_mapping.csv\"),\n",
    "    os.path.join(data_test, \"protein_chain_cluster_mapping.csv\"),\n",
    "]\n",
    "\n",
    "sampler = WeightedPDBSampler(\n",
    "    chain_mapping_paths=chain_mapping_paths,\n",
    "    interface_mapping_path=interface_mapping_path,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "dataset = PDBDataset(\n",
    "    folder=os.path.join(\"/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data\", \"merged_mmcifs\"), \n",
    "    sampler=sampler, sample_type=\"default\", crop_size=128\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDBInput(mmcif_filepath='/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data/merged_mmcifs/21/521p-assembly1.cif', biomol=None, chains=('A', 'B'), cropping_config=None, msa_dir=None, templates_dir=None, add_atom_ids=False, add_atompair_ids=False, directed_bonds=False, training=None, distillation=False, resolution=None, max_msas_per_chain=None, max_templates_per_chain=None, num_templates_per_chain=None, kalign_binary_path=None, extract_atom_feats_fn=<function default_extract_atom_feats_fn at 0x7f2a62caa4d0>, extract_atompair_feats_fn=<function default_extract_atompair_feats_fn at 0x7f2a62caa560>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n"
     ]
    }
   ],
   "source": [
    "# pdb_input = dataset[0]\n",
    "# print(11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDBInput(mmcif_filepath='/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data/merged_mmcifs/sl/6sl5-assembly1.cif', biomol=None, chains=('E', 'F'), cropping_config=None, msa_dir=None, templates_dir=None, add_atom_ids=False, add_atompair_ids=False, directed_bonds=False, training=None, resolution=None, max_msas_per_chain=None, extract_atom_feats_fn=<function default_extract_atom_feats_fn at 0x7f0acc654820>, extract_atompair_feats_fn=<function default_extract_atompair_feats_fn at 0x7f0acc6548b0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_input = dataset[5]\n",
    "mol_input = pdb_input_to_molecule_input(pdb_input=pdb_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:6mw0-assembly1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from alphafold3_pytorch.utils.utils import default, exists, first\n",
    "error_cnt=0\n",
    "for i in range(len(dataset)):\n",
    "    filepath = dataset[i].mmcif_filepath\n",
    "    file_id = os.path.splitext(os.path.basename(filepath))[0] if exists(filepath) else None\n",
    "    if file_id !='2mtz-assembly1':\n",
    "        continue\n",
    "    print(f\"Processing:{file_id}\")\n",
    "    mol_input = pdb_input_to_molecule_input(pdb_input=dataset[i])\n",
    "    atom_input = molecule_to_atom_input(mol_input)\n",
    "    batched_atom_input = collate_inputs_to_batched_atom_input([atom_input], atoms_per_window=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass data:0 | 214d-assembly1\n",
      "pass data:1 | 5vvr-assembly1\n",
      "pass data:2 | 1n4r-assembly1\n",
      "pass data:3 | 108d-assembly1\n",
      "pass data:4 | 4joe-assembly1\n",
      "Error in 5:5my9-assembly1\n",
      "Exception: index 493 is out of bounds for dimension 0 with size 493\n",
      "pass data:6 | 308d-assembly1\n",
      "pass data:7 | 207d-assembly1\n",
      "pass data:8 | 5nj8-assembly1\n",
      "pass data:9 | 209d-assembly1\n",
      "pass data:10 | 1mt8-assembly1\n",
      "pass data:11 | 315d-assembly1\n",
      "pass data:12 | 7nhl-assembly1\n",
      "pass data:13 | 207d-assembly1\n",
      "pass data:14 | 220l-assembly1\n",
      "Error in 15:7nhm-assembly1\n",
      "Exception: The size of tensor a (6465) must match the size of tensor b (6478) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "error_cnt=0\n",
    "for i in range(len(dataset)):\n",
    "    data = dataset[i]\n",
    "    filepath = data.mmcif_filepath\n",
    "    file_id = os.path.splitext(os.path.basename(filepath))[0] if exists(filepath) else None\n",
    "    try:\n",
    "        mol_input = pdb_input_to_molecule_input(pdb_input=data)\n",
    "        print(f\"pass data:{i} | {file_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {i}:{file_id}\")\n",
    "        print(f'Exception: {e}')\n",
    "        error_cnt+=1\n",
    "        if error_cnt ==2:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_input = pdb_input_to_molecule_input(pdb_input=dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_input = molecule_to_atom_input(mol_input)\n",
    "batched_atom_input = collate_inputs_to_batched_atom_input([atom_input], atoms_per_window=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom_inputs torch.Size([1, 25240, 3])\n",
      "molecule_ids torch.Size([1, 3176])\n",
      "molecule_atom_lens torch.Size([1, 3176])\n",
      "atompair_inputs torch.Size([1, 935, 27, 54, 5])\n",
      "additional_molecule_feats torch.Size([1, 3176, 5])\n",
      "is_molecule_types torch.Size([1, 3176, 5])\n",
      "is_molecule_mod torch.Size([1, 3176, 4])\n",
      "additional_msa_feats torch.Size([1, 1, 3176, 2])\n",
      "additional_token_feats torch.Size([1, 3176, 33])\n",
      "templates None\n",
      "msa None\n",
      "token_bonds torch.Size([1, 3176, 3176])\n",
      "atom_ids None\n",
      "atom_parent_ids torch.Size([1, 25240])\n",
      "atompair_ids None\n",
      "template_mask None\n",
      "msa_mask None\n",
      "atom_pos torch.Size([1, 25240, 3])\n",
      "missing_atom_mask torch.Size([1, 25240])\n",
      "molecule_atom_indices torch.Size([1, 3176])\n",
      "distogram_atom_indices torch.Size([1, 3176])\n",
      "atom_indices_for_frame torch.Size([1, 3176, 3])\n",
      "distance_labels None\n",
      "resolved_labels torch.Size([1, 25240])\n",
      "resolution torch.Size([1])\n",
      "chains torch.Size([1, 2])\n",
      "filepath ('/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data/merged_mmcifs/jn/4jn8-assembly1.cif',)\n",
      "('/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data/merged_mmcifs/jn/4jn8-assembly1.cif',)\n"
     ]
    }
   ],
   "source": [
    "for key,value in batched_atom_input.dict().items():\n",
    "    try:\n",
    "        print(key,value.shape)\n",
    "    except:\n",
    "        print(key,value)\n",
    "print(batched_atom_input.dict()['filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.81 GiB. GPU 0 has a total capacity of 79.35 GiB of which 3.47 GiB is free. Process 7256 has 75.88 GiB memory in use. Of the allocated memory 70.26 GiB is allocated by PyTorch, and 5.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m alphafold3 \u001b[38;5;241m=\u001b[39m Alphafold3(\n\u001b[1;32m      2\u001b[0m         dim_atom_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      3\u001b[0m         dim_atompair_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         ),\n\u001b[1;32m     16\u001b[0m     )\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     17\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batched_atom_input\u001b[38;5;241m.\u001b[39mmodel_forward_dict()\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43malphafold3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:6494\u001b[0m, in \u001b[0;36mAlphafold3.forward\u001b[0;34m(self, atom_inputs, atompair_inputs, additional_molecule_feats, is_molecule_types, molecule_atom_lens, molecule_ids, additional_msa_feats, additional_token_feats, atom_ids, atompair_ids, is_molecule_mod, atom_mask, missing_atom_mask, atom_indices_for_frame, valid_atom_indices_for_frame, atom_parent_ids, token_bonds, msa, msa_mask, templates, template_mask, num_recycling_steps, diffusion_add_bond_loss, diffusion_add_smooth_lddt_loss, distogram_atom_indices, molecule_atom_indices, num_sample_steps, atom_pos, distance_labels, resolved_labels, resolution, return_loss_breakdown, return_loss, return_all_diffused_atom_pos, return_confidence_head_logits, return_distogram_head_logits, num_rollout_steps, rollout_show_tqdm_pbar, detach_when_recycling, min_conf_resolution, max_conf_resolution, hard_validate)\u001b[0m\n\u001b[1;32m   6490\u001b[0m         pairformer \u001b[38;5;241m=\u001b[39m partial(checkpoint, pairformer)\n\u001b[1;32m   6492\u001b[0m     \u001b[38;5;66;03m# main attention trunk (pairformer)\u001b[39;00m\n\u001b[0;32m-> 6494\u001b[0m     single, pairwise \u001b[38;5;241m=\u001b[39m \u001b[43mpairformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6495\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_repr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpairwise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m   6498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6500\u001b[0m \u001b[38;5;66;03m# determine whether to return loss if any labels were to be passed in\u001b[39;00m\n\u001b[1;32m   6501\u001b[0m \u001b[38;5;66;03m# otherwise will sample the atomic coordinates\u001b[39;00m\n\u001b[1;32m   6503\u001b[0m atom_pos_given \u001b[38;5;241m=\u001b[39m exists(atom_pos)\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:1483\u001b[0m, in \u001b[0;36mPairformerStack.forward\u001b[0;34m(self, single_repr, pairwise_repr, mask)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     to_layers_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_layers\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# main transformer block layers\u001b[39;00m\n\u001b[0;32m-> 1483\u001b[0m single_repr, pairwise_repr \u001b[38;5;241m=\u001b[39m \u001b[43mto_layers_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_repr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_repr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;66;03m# splice out registers\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_registers:\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:1389\u001b[0m, in \u001b[0;36mPairformerStack.to_layers\u001b[0;34m(self, single_repr, pairwise_repr, mask)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_depth):\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m   1384\u001b[0m         pairwise_block,\n\u001b[1;32m   1385\u001b[0m         pair_bias_attn,\n\u001b[1;32m   1386\u001b[0m         single_transition\n\u001b[1;32m   1387\u001b[0m     ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m-> 1389\u001b[0m         pairwise_repr \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m         single_repr \u001b[38;5;241m=\u001b[39m pair_bias_attn(single_repr, pairwise_repr \u001b[38;5;241m=\u001b[39m pairwise_repr, mask \u001b[38;5;241m=\u001b[39m mask) \u001b[38;5;241m+\u001b[39m single_repr\n\u001b[1;32m   1392\u001b[0m         single_repr \u001b[38;5;241m=\u001b[39m single_transition(single_repr) \u001b[38;5;241m+\u001b[39m single_repr\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:903\u001b[0m, in \u001b[0;36mPairwiseBlock.forward\u001b[0;34m(self, pairwise_repr, mask)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;129m@typecheck\u001b[39m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    901\u001b[0m     mask: Bool[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    902\u001b[0m ):\n\u001b[0;32m--> 903\u001b[0m     pairwise_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtri_mult_outgoing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairwise_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m pairwise_repr\n\u001b[1;32m    904\u001b[0m     pairwise_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_mult_incoming(pairwise_repr, mask \u001b[38;5;241m=\u001b[39m mask) \u001b[38;5;241m+\u001b[39m pairwise_repr\n\u001b[1;32m    905\u001b[0m     pairwise_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtri_attn_starting(pairwise_repr, mask \u001b[38;5;241m=\u001b[39m mask) \u001b[38;5;241m+\u001b[39m pairwise_repr\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:579\u001b[0m, in \u001b[0;36mPreLayerNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;129m@typecheck\u001b[39m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     x: Float[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... n d\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    576\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... n d\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    578\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/code/AF3/alphafold3-pytorch/alphafold3_pytorch/alphafold3.py:710\u001b[0m, in \u001b[0;36mTriangleMultiplication.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    707\u001b[0m     left \u001b[38;5;241m=\u001b[39m left \u001b[38;5;241m*\u001b[39m mask\n\u001b[1;32m    708\u001b[0m     right \u001b[38;5;241m=\u001b[39m right \u001b[38;5;241m*\u001b[39m mask\n\u001b[0;32m--> 710\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmix_einsum_eq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out_norm(out)\n\u001b[1;32m    714\u001b[0m out_gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_gate(x)\u001b[38;5;241m.\u001b[39msigmoid()\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/einops/einops.py:907\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    905\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    906\u001b[0m pattern \u001b[38;5;241m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/einops/_backends.py:287\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, pattern, *x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern, \u001b[38;5;241m*\u001b[39mx):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/workspace/chengkaihui/conda_envs/af3/lib/python3.10/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.81 GiB. GPU 0 has a total capacity of 79.35 GiB of which 3.47 GiB is free. Process 7256 has 75.88 GiB memory in use. Of the allocated memory 70.26 GiB is allocated by PyTorch, and 5.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "alphafold3 = Alphafold3(\n",
    "        dim_atom_inputs=3,\n",
    "        dim_atompair_inputs=5,\n",
    "        atoms_per_window=27,\n",
    "        dim_template_feats=44,\n",
    "        num_dist_bins=38,\n",
    "        confidence_head_kwargs=dict(pairformer_depth=1),\n",
    "        template_embedder_kwargs=dict(pairformer_stack_depth=1),\n",
    "        msa_module_kwargs=dict(depth=1),\n",
    "        pairformer_stack=dict(depth=2),\n",
    "        diffusion_module_kwargs=dict(\n",
    "            atom_encoder_depth=1,\n",
    "            token_transformer_depth=1,\n",
    "            atom_decoder_depth=1,\n",
    "        ),\n",
    "    ).cuda()\n",
    "input_data = {k: v.cuda() if v is not None else v for k, v in batched_atom_input.model_forward_dict().items()}\n",
    "\n",
    "loss = alphafold3(**input_data)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9960, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# folder= os.path.join(\"/cpfs01/projects-HDD/cfff-6f3a36a0cd1e_HDD/public/protein/datasets/AF3/data/pdb_data\", \"test_mmcifs\")\n",
    "# if isinstance(folder, str):\n",
    "#     folder = Path(folder)\n",
    "# sampler_pdb_ids = set(sampler.mappings.get_column(\"pdb_id\").to_list())\n",
    "# files = {\n",
    "#     os.path.splitext(os.path.basename(filepath.name))[0]: filepath\n",
    "#     for filepath in folder.glob(os.path.join(\"**\", \"*.cif\"))\n",
    "#     if os.path.splitext(os.path.basename(filepath.name))[0] in sampler_pdb_ids\n",
    "# }\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173540\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
